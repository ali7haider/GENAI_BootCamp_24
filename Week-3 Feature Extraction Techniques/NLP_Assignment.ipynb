{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## **Gen-AI Bootcamp 24** \n",
    "_________________________________________________________________________\n",
    "\n",
    "## **Part 1: Text Collection and Loading**\n",
    "**Objective:** *Collect and load a text dataset from a selected domain into a suitable format for\n",
    "processing.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Domain**: *Social Media* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Kaggle Dataset**: *https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Loading Dataset**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162980, 2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "df = pd.read_csv('Twitter_Data.csv')\n",
    "# Print the shape of the DataFrame\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Displaying the first few rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category\n",
       "0  when modi promised “minimum government maximum...      -1.0\n",
       "1  talk all the nonsense and continue all the dra...       0.0\n",
       "2  what did just say vote for modi  welcome bjp t...       1.0\n",
       "3  asking his supporters prefix chowkidar their n...       1.0\n",
       "4  answer who among these the most powerful world...       1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first five rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________\n",
    "## **Part 2: Text Preprocessing**\n",
    "**Objective:** *Gain hands-on experience with text preprocessing techniques.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Import the Necessary Libraries and Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\Digital\n",
      "[nltk_data]     Zone\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Digital\n",
      "[nltk_data]     Zone\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Digital\n",
      "[nltk_data]     Zone\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Digital\n",
      "[nltk_data]     Zone\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Digital\n",
      "[nltk_data]     Zone\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')  # Download Brown Data\n",
    "nltk.download('punkt') # Download Punkt Data\n",
    "nltk.download('stopwords') # Download Stopwords Data\n",
    "nltk.download('wordnet') # Download WordNet Data\n",
    "nltk.download('omw-1.4')  # Download WordNet Data\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Load the Brown Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: 0         when modi promised “minimum government maximum...\n",
      "1         talk all the nonsense and continue all the dra...\n",
      "2         what did just say vote for modi  welcome bjp t...\n",
      "3         asking his supporters prefix chowkidar their n...\n",
      "4         answer who among these the most powerful world...\n",
      "                                ...                        \n",
      "162975    why these 456 crores paid neerav modi not reco...\n",
      "162976    dear rss terrorist payal gawar what about modi...\n",
      "162977    did you co\n"
     ]
    }
   ],
   "source": [
    "# Load the text from the 'news' category of the Brown Corpus\n",
    "# text = ' '.join(brown.words(categories='science_fiction'))\n",
    "text = str(df['clean_text'])\n",
    "print(\"Original Text:\", text[:500])  # Print the first 500 characters for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Tokenization**\n",
    "Tokenization splits the text into individual words and sentences.\n",
    "\n",
    "**Impact of Tokenization:**\n",
    "* Sentence Tokenization: Breaks down the text into manageable units (sentences) for further processing.\n",
    "* Word Tokenization: Provides the basic units (words) needed for subsequent analysis steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 sentences: ['0         when modi promised “minimum government maximum...\\n1         talk all the nonsense and continue all the dra...\\n2         what did just say vote for modi  welcome bjp t...\\n3         asking his supporters prefix chowkidar their n...\\n4         answer who among these the most powerful world...\\n                                ...                        \\n162975    why these 456 crores paid neerav modi not reco...\\n162976    dear rss terrorist payal gawar what about modi...\\n162977    did you cover her interaction forum where she ...\\n162978    there big project came into india modi dream p...\\n162979    have you ever listen about like gurukul where ...\\nName: clean_text, Length: 162980, dtype: object']\n",
      "First 20 words: ['0', 'when', 'modi', 'promised', '“', 'minimum', 'government', 'maximum', '...', '1', 'talk', 'all', 'the', 'nonsense', 'and', 'continue', 'all', 'the', 'dra', '...']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"First 5 sentences:\", sentences[:5])\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "print(\"First 20 words:\", words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Stemming**\n",
    "Stemming reduces words to their root form, stripping suffixes.\n",
    "\n",
    "**Impact of Stemming:**\n",
    "* Reduction of Variants: Words like \"running,\" \"runner,\" and \"ran\" are reduced to \"run,\" which simplifies the text and reduces complexity.\n",
    "* Potential Loss of Meaning: Sometimes, stemming can strip too much, losing the actual meaning of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 stemmed words: ['0', 'when', 'modi', 'promis', '“', 'minimum', 'govern', 'maximum', '...', '1', 'talk', 'all', 'the', 'nonsens', 'and', 'continu', 'all', 'the', 'dra', '...']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming to each word\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(\"First 20 stemmed words:\", stemmed_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5: Lemmatization**\n",
    "Lemmatization reduces words to their base or dictionary form, considering the context.\n",
    "\n",
    "**Impact of Lemmatization:**\n",
    "* Context-Aware Reduction: More accurate than stemming, as it considers the part of speech and context.\n",
    "* Improved Meaning Preservation: Maintains the integrity of the words better than stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 lemmatized words: ['0', 'when', 'modi', 'promis', '“', 'minimum', 'govern', 'maximum', '...', '1', 'talk', 'all', 'the', 'nonsens', 'and', 'continu', 'all', 'the', 'dra', '...']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization to each word\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
    "print(\"First 20 lemmatized words:\", lemmatized_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6: Stop Word Removal**\n",
    "Stop words are common words (e.g., \"the,\" \"is\") that may not add significant meaning to text analysis.\n",
    "\n",
    "**Impact of Stop Word Removal:**\n",
    "* Noise Reduction: Eliminates common but uninformative words, reducing the size of the text data.\n",
    "* Focus on Meaningful Words: Enhances the focus on significant words that contribute more to the text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words after stop word removal: ['0', 'modi', 'promis', '“', 'minimum', 'govern', 'maximum', '...', '1', 'talk', 'nonsens', 'continu', 'dra', '...', '2', 'say', 'vote', 'modi', 'welcom', 'bjp']\n"
     ]
    }
   ],
   "source": [
    "# Get the list of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words from the tokenized words\n",
    "filtered_words = [word for word in lemmatized_words if word.lower() not in stop_words and word not in string.punctuation]\n",
    "print(\"First 20 words after stop word removal:\", filtered_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________\n",
    "## **Part 3: Feature Extraction Techniques**\n",
    "**Objective:** *Understand and apply text data transformation into machine-readable vectors.*\n",
    "\n",
    "Whenever we apply any algorithm in NLP, it works on numbers. We cannot directly feed our text into that algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Bag-of-words**\n",
    "Bag of Words model is used to preprocess the text by converting it into a bag of words, which keeps a count of the total occurrences of most frequently used words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step-by-Step Example**\n",
    "Consider the following three simple documents:\n",
    "1. \"the cat sat on the mat\"\n",
    "2. \"the dog barked at the cat\"\n",
    "3. \"the cat chased the mouse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 1: Tokenization**\n",
    "Split each document into words (tokens)\n",
    "\n",
    "    Document 1: [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "\n",
    "    Document 2: [\"the\", \"dog\", \"barked\", \"at\", \"the\", \"cat\"]\n",
    "\n",
    "    Document 3: [\"the\", \"cat\", \"chased\", \"the\", \"mouse\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 2: Vocabulary Creation**\n",
    "Combine all tokens from all documents and identify unique words to create the vocabulary\n",
    "\n",
    "    Vocabulary: [\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"barked\", \"at\", \"chased\", \"mouse\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 3: Vectorization**\n",
    "Create a vector for each document based on the vocabulary. Each position in the vector corresponds to a word in the vocabulary, and the value is the count of that word in the document.\n",
    "\n",
    "For the given vocabulary [\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"barked\", \"at\", \"chased\", \"mouse\"], let's create the vectors for each document:\n",
    "\n",
    "![Description](images/docum1.png)\n",
    "![Description](images/docum2.png)\n",
    "![Description](images/docum3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
    "TF-IDF, or Term Frequency-Inverse Document Frequency, is a numerical statistic that reflects how important a word is to a document in a collection or corpus. It helps to highlight important words in a document while reducing the impact of commonly occurring words that might be less informative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2.1 Term Frequency (TF)**\n",
    "Term Frequency measures how frequently a term appears in a document. It is calculated as the number of times a term appears in a document divided by the total number of terms in the document.\n",
    "\n",
    "![Description](images/tf.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2.2 Inverse Document Frequency (IDF)**\n",
    "Inverse Document Frequency (IDF) measures how important a term is by comparing the number of documents that contain the term to the total number of documents. If a term appears in many documents, its IDF value will be low.\n",
    "\n",
    "![Description](images/idf.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2.3 TF-IDF**\n",
    "TF-IDF combines both measures to give a score that represents the importance of a term in a document relative to the entire corpus.\n",
    "\n",
    "![Description](images/tf-idf.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2.4 TF_IDF Example**\n",
    "Step 1 : Calculate Term Frequency (TF)\n",
    "\n",
    "Term Frequency (TF) measures how frequently a term appears in a document. It is calculated as the number of times a term appears in a document divided by the total number of terms in the document.\n",
    "\n",
    "![Description](images/tfnew.png)\n",
    "\n",
    "Step 2: Calculate Inverse Document Frequency (IDF)\n",
    "\n",
    "Inverse Document Frequency (IDF) measures how important a term is across the entire corpus. It is calculated as the logarithm of the total number of documents divided by the number of documents containing the term.\n",
    "\n",
    "![Description](images/idfnew.png)\n",
    "\n",
    "Step 3: Calculate TF-IDF\n",
    "\n",
    "TF-IDF is the product of TF and IDF for each term in each document.\n",
    "\n",
    "![Description](images/tf-idfnew.png)\n",
    "\n",
    "Summary of Results\n",
    "\n",
    "![Description](images/tfidfres.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. n-grams**\n",
    "The n-gram model is an extension of the Bag-of-Words model that considers sequences of n words (called n-grams) instead of individual words (unigrams). This allows the model to capture some of the context and order of words in the text. An n-gram is a contiguous sequence of n items from a given text.\n",
    "#### Types of n-grams\n",
    "1. Unigram: Single word (n=1)\n",
    "2. Bigram: Sequence of two words (n=2)\n",
    "3. Trigram: Sequence of three words (n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example**\n",
    "Consider the following three simple documents:\n",
    "1. \"the cat sat on the mat\"\n",
    "2. \"the dog barked at the cat\"\n",
    "3. \"the cat chased the mouse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 1: Tokenization**\n",
    "    Document 1: [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "\n",
    "    Document 2: [\"the\", \"dog\", \"barked\", \"at\", \"the\", \"cat\"]\n",
    "\n",
    "    Document 3: [\"the\", \"cat\", \"chased\", \"the\", \"mouse\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 2: Generate Bigrams:**\n",
    "    Document 1: [\"the cat\", \"cat sat\", \"sat on\", \"on the\", \"the mat\"]\n",
    "\n",
    "    Document 2: [\"the dog\", \"dog barked\", \"barked at\", \"at the\", \"the cat\"]\n",
    "\n",
    "    Document 3: [\"the cat\", \"cat chased\", \"chased the\", \"the mouse\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 3: Vocabulary Creation:**\n",
    "Combine all bigrams from all documents and find unique bigrams\n",
    "\n",
    "    Vocabulary: [\"the cat\", \"cat sat\", \"sat on\", \"on the\", \"the mat\", \"the dog\", \"dog barked\", \"barked at\", \"at the\", \"cat chased\", \"chased the\", \"the mouse\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 4: Vocabulary Creation:**\n",
    "Create a vector for each document based on the vocabulary. Each position in the vector corresponds to a bigram in the vocabulary, and the value is the count of that bigram in the document.\n",
    "\n",
    "For the given vocabulary [\"the cat\", \"cat sat\", \"sat on\", \"on the\", \"the mat\", \"the dog\", \"dog barked\", \"barked at\", \"at the\", \"cat chased\", \"chased the\", \"the mouse\"], let's create the vectors for each document:\n",
    "\n",
    "![Description](images/docu1.png)\n",
    "![Description](images/docu2.png)\n",
    "![Description](images/docu3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________\n",
    "## **Part 5: Model Training and Evaluation**\n",
    "**Objective:** *Understand RNNs and their ability to handle sequence data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Introduction**\n",
    "\n",
    "##### **Sequential Data**\n",
    "Sequential data refers to data where the order of the elements is significant and the sequence of elements matters. This type of data is common in many real-world applications, such as:\n",
    "\n",
    "1. Time series data: Stock prices, weather data, sensor readings, etc.\n",
    "2. Natural language processing: Sentences in a text, where the order of words is important.\n",
    "3. Biological sequences: DNA or protein sequences, where the order of nucleotides or amino acids matters.   \n",
    "##### **1. Recurrent Neural Networks (RNNs)**\n",
    "\n",
    "\n",
    "##### **Overview:**\n",
    "\n",
    "\n",
    "1. RNNs are designed to handle sequential data by maintaining a hidden state that captures information from previous time steps.\n",
    "2. They are used for tasks where the order of data points matters, such as time series forecasting, language modeling, and speech recognition.\n",
    "3. Unlike traditional neural networks, RNNs have connections that form directed cycles, which allow them to maintain a state and capture temporal dependencies.\n",
    "\n",
    "##### **Structure:**\n",
    "RNNs have loops that allow information to persist. They process input sequences one step at a time, maintaining a hidden state that is updated at each time step.\n",
    "\n",
    "\n",
    "##### **Problems:**\n",
    "Vanishing Gradient: Gradients can become very small, making it difficult for the model to learn long-term dependencies.\n",
    "\n",
    "Exploding Gradient: Gradients can become very large, leading to unstable training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2. Long Short-Term Memory (LSTM)**\n",
    "\n",
    "\n",
    "##### **Overview:**\n",
    "\n",
    "\n",
    "1. LSTM networks are a type of RNN designed to capture long-term dependencies. They include mechanisms called gates to regulate the flow of information.\n",
    "2. LSTMs are used for the same tasks as RNNs but perform better when the sequence length is long.\n",
    "\n",
    "\n",
    "##### **Structure:**\n",
    "1. Forget Gate: Decides what information to throw away from the cell state.\n",
    "2. Input Gate: Decides which new information to store in the cell state.\n",
    "3. Output Gate: Decides what part of the cell state to output.\n",
    "\n",
    "\n",
    "##### **Problems:**\n",
    "Complexity: LSTMs are more complex and computationally expensive compared to simple RNNs.\n",
    "\n",
    "Training Time: Longer training times due to their complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **3. Gated Recurrent Unit (GRU)**\n",
    "\n",
    "\n",
    "##### **Overview:**\n",
    "\n",
    "\n",
    "1. GRUs are a simplified version of LSTMs that combine the forget and input gates into a single update gate.\n",
    "2. They aim to provide similar benefits to LSTMs but with fewer parameters and less computational complexity.\n",
    "\n",
    "\n",
    "##### **Structure:**\n",
    "1. Reset Gate: Decides how much of the past information to forget.\n",
    "2. Update Gate: Decides how much of the new information to use to update the cell state.\n",
    "\n",
    "\n",
    "##### **Problems:**\n",
    "Long-Term Dependencies: While GRUs can capture long-term dependencies, they might not be as effective as LSTMs in very long sequences.\n",
    "\n",
    "Complexity: More complex than simple RNNs but simpler than LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Comparison of RNN, LSTM, and GRU:**\n",
    "\n",
    "\n",
    "##### **RNN:**\n",
    "\n",
    "Pros: Simpler architecture, faster computation.\n",
    "\n",
    "Cons: Struggles with long-term dependencies due to vanishing/exploding gradient problems.\n",
    "\n",
    "\n",
    "##### **LSTM::**\n",
    "Pros: Effective at capturing long-term dependencies, addresses vanishing gradient problem.\n",
    "\n",
    "Cons: More complex, longer training time.\n",
    "\n",
    "\n",
    "##### **GRU::**\n",
    "Pros: Simpler than LSTMs, faster training, effective at capturing long-term dependencies.\n",
    "\n",
    "Cons: Slightly less expressive than LSTMs but generally performs similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Applications of RNNs**\n",
    "1. Time series prediction\n",
    "2. Natural language processing\n",
    "3. Speech recognition\n",
    "4. Video processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

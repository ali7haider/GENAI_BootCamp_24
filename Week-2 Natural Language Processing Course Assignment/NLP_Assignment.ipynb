{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Gen-AI Bootcamp 24** \n",
    "\n",
    "_________________________________________________________________________\n",
    "\n",
    "## **Part 1: Text Collection and Loading**\n",
    "**Objective:** *Collect and load a text dataset from a selected domain into a suitable format for\n",
    "processing.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Domain**: *Automotive* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Kaggle Dataset**: *https://www.kaggle.com/datasets/ankkur13/edmundsconsumer-car-ratings-and-reviews?select=Scraped_Car_Review_dodge.csv*\n",
    "This is a dataset containing consumer's thought and the star rating of car manufacturer/model/type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Loading Dataset**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1020, 7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "df = pd.read_csv('Scraped_Car_Review_dodge.csv')\n",
    "# Print the shape of the DataFrame\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Displaying the first few rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Author_Name</th>\n",
       "      <th>Vehicle_Title</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>on 10/13/05 15:30 PM (PDT)</td>\n",
       "      <td>roadking</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Great delivery vehicle</td>\n",
       "      <td>It's been a great delivery vehicle for my caf...</td>\n",
       "      <td>4.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>on 07/17/05 21:59 PM (PDT)</td>\n",
       "      <td>Mark</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 3500 3dr Ext Van (5.2...</td>\n",
       "      <td>Disappointmnet</td>\n",
       "      <td>Bought this car as a commuter vehicle for a v...</td>\n",
       "      <td>2.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>on 07/16/02 00:00 AM (PDT)</td>\n",
       "      <td>Tom Sheer</td>\n",
       "      <td>2002 Dodge Ram Cargo Van 3500 Maxi 3dr Ext Van...</td>\n",
       "      <td>Sweet van</td>\n",
       "      <td>This van rocks its the best, lots of room. I ...</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>on 12/29/07 21:57 PM (PST)</td>\n",
       "      <td>Keven Smith</td>\n",
       "      <td>2001 Dodge Ram Cargo Van 2500 Maxi 3dr Ext Van...</td>\n",
       "      <td>Keven Smith</td>\n",
       "      <td>Great work vehicle. Drives nice. has lots of ...</td>\n",
       "      <td>4.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>on 02/09/05 18:52 PM (PST)</td>\n",
       "      <td>VanMan</td>\n",
       "      <td>2001 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...</td>\n",
       "      <td>Not what Dodge used to be</td>\n",
       "      <td>Good solid frame and suspension.  Well equipp...</td>\n",
       "      <td>2.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                  Review_Date   Author_Name  \\\n",
       "0   0   on 10/13/05 15:30 PM (PDT)     roadking    \n",
       "1   1   on 07/17/05 21:59 PM (PDT)         Mark    \n",
       "2   2   on 07/16/02 00:00 AM (PDT)    Tom Sheer    \n",
       "3   3   on 12/29/07 21:57 PM (PST)  Keven Smith    \n",
       "4   4   on 02/09/05 18:52 PM (PST)       VanMan    \n",
       "\n",
       "                                       Vehicle_Title  \\\n",
       "0  2002 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...   \n",
       "1  2002 Dodge Ram Cargo Van 3500 3dr Ext Van (5.2...   \n",
       "2  2002 Dodge Ram Cargo Van 3500 Maxi 3dr Ext Van...   \n",
       "3  2001 Dodge Ram Cargo Van 2500 Maxi 3dr Ext Van...   \n",
       "4  2001 Dodge Ram Cargo Van 1500 3dr Van (3.9L 6c...   \n",
       "\n",
       "                Review_Title  \\\n",
       "0     Great delivery vehicle   \n",
       "1             Disappointmnet   \n",
       "2                  Sweet van   \n",
       "3                Keven Smith   \n",
       "4  Not what Dodge used to be   \n",
       "\n",
       "                                              Review  Rating  \n",
       "0   It's been a great delivery vehicle for my caf...   4.625  \n",
       "1   Bought this car as a commuter vehicle for a v...   2.125  \n",
       "2   This van rocks its the best, lots of room. I ...   5.000  \n",
       "3   Great work vehicle. Drives nice. has lots of ...   4.500  \n",
       "4   Good solid frame and suspension.  Well equipp...   2.875  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first five rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________\n",
    "## **Part 2: Text Preprocessing**\n",
    "**Objective:** *Gain hands-on experience with text preprocessing techniques.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Import the Necessary Libraries and Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')  # Download Brown Data\n",
    "nltk.download('punkt') # Download Punkt Data\n",
    "nltk.download('stopwords') # Download Stopwords Data\n",
    "nltk.download('wordnet') # Download WordNet Data\n",
    "nltk.download('omw-1.4')  # Download WordNet Data\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Load the Brown Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: 0        It's been a great delivery vehicle for my caf...\n",
      "1        Bought this car as a commuter vehicle for a v...\n",
      "2        This van rocks its the best, lots of room. I ...\n",
      "3        Great work vehicle. Drives nice. has lots of ...\n",
      "4        Good solid frame and suspension.  Well equipp...\n",
      "                              ...                        \n",
      "1015     I am constantly having to take this car into ...\n",
      "1016     I have enjoyed my Neon from day one, I bought...\n",
      "1017     This is my 3rd Neon (1995 s\n"
     ]
    }
   ],
   "source": [
    "# Load the text from the 'news' category of the Brown Corpus\n",
    "# text = ' '.join(brown.words(categories='science_fiction'))\n",
    "text = str(df['Review'])\n",
    "print(\"Original Text:\", text[:500])  # Print the first 500 characters for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Tokenization**\n",
    "Tokenization splits the text into individual words and sentences.\n",
    "\n",
    "**Impact of Tokenization:**\n",
    "* Sentence Tokenization: Breaks down the text into manageable units (sentences) for further processing.\n",
    "* Word Tokenization: Provides the basic units (words) needed for subsequent analysis steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 sentences: [\"0        It's been a great delivery vehicle for my caf...\\n1        Bought this car as a commuter vehicle for a v...\\n2        This van rocks its the best, lots of room.\", 'I ...\\n3        Great work vehicle.', 'Drives nice.', 'has lots of ...\\n4        Good solid frame and suspension.', 'Well equipp...\\n                              ...                        \\n1015     I am constantly having to take this car into ...\\n1016     I have enjoyed my Neon from day one, I bought...\\n1017     This is my 3rd Neon (1995 sedan & 1999 sport ...\\n1018     I bought this car less than two years ago wit...\\n1019     Less than 4 years after purchase this car has...\\nName: Review, Length: 1020, dtype: object']\n",
      "First 20 words: ['0', 'It', \"'s\", 'been', 'a', 'great', 'delivery', 'vehicle', 'for', 'my', 'caf', '...', '1', 'Bought', 'this', 'car', 'as', 'a', 'commuter', 'vehicle']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"First 5 sentences:\", sentences[:5])\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "print(\"First 20 words:\", words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Stemming**\n",
    "Stemming reduces words to their root form, stripping suffixes.\n",
    "\n",
    "**Impact of Stemming:**\n",
    "* Reduction of Variants: Words like \"running,\" \"runner,\" and \"ran\" are reduced to \"run,\" which simplifies the text and reduces complexity.\n",
    "* Potential Loss of Meaning: Sometimes, stemming can strip too much, losing the actual meaning of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 stemmed words: ['0', 'it', \"'s\", 'been', 'a', 'great', 'deliveri', 'vehicl', 'for', 'my', 'caf', '...', '1', 'bought', 'thi', 'car', 'as', 'a', 'commut', 'vehicl']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming to each word\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(\"First 20 stemmed words:\", stemmed_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5: Lemmatization**\n",
    "Lemmatization reduces words to their base or dictionary form, considering the context.\n",
    "\n",
    "**Impact of Lemmatization:**\n",
    "* Context-Aware Reduction: More accurate than stemming, as it considers the part of speech and context.\n",
    "* Improved Meaning Preservation: Maintains the integrity of the words better than stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 lemmatized words: ['0', 'it', \"'s\", 'been', 'a', 'great', 'deliveri', 'vehicl', 'for', 'my', 'caf', '...', '1', 'bought', 'thi', 'car', 'a', 'a', 'commut', 'vehicl']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization to each word\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
    "print(\"First 20 lemmatized words:\", lemmatized_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6: Stop Word Removal**\n",
    "Stop words are common words (e.g., \"the,\" \"is\") that may not add significant meaning to text analysis.\n",
    "\n",
    "**Impact of Stop Word Removal:**\n",
    "* Noise Reduction: Eliminates common but uninformative words, reducing the size of the text data.\n",
    "* Focus on Meaningful Words: Enhances the focus on significant words that contribute more to the text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words after stop word removal: ['0', \"'s\", 'great', 'deliveri', 'vehicl', 'caf', '...', '1', 'bought', 'thi', 'car', 'commut', 'vehicl', 'v', '...', '2', 'thi', 'van', 'rock', 'best']\n"
     ]
    }
   ],
   "source": [
    "# Get the list of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words from the tokenized words\n",
    "filtered_words = [word for word in lemmatized_words if word.lower() not in stop_words and word not in string.punctuation]\n",
    "print(\"First 20 words after stop word removal:\", filtered_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________\n",
    "## **Part 3: Feature Extraction Techniques**\n",
    "**Objective:** *Understand and apply text data transformation into machine-readable vectors.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **One-Hot Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bag-of-words**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
